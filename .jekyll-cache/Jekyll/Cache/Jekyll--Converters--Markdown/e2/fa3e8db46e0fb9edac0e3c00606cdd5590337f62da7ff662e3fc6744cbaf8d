I"('<p>Let’s start out by looking at a screenshot from <a href="https://www.youtube.com/watch?v=5yJ_QLec0Lc" target="_blank">Jon Krohn’s excellent video</a> on big O notation which I would recommend watching if you are totally unfamiliar with this topic:</p>

<p><img src="https://i.imgur.com/pUzkOKi.png" alt="big o notations arranged on a grid by Jon Krohn" /></p>

<p>Many lessons on big O will show something similar to this with the idea of conveying one simple truth: there exists a spectrum of big O notations that can be arranged from the least efficient scaling to the most efficient scaling, and in our case above, O(n!) would scale with the least amount of efficiency while O(log n) would scale with the most efficiency.   In this way, if I have two computer programming functions that do the exact same thing, and I must choose to use one of them in my software build, and all I know about these two computer programming functions is that one of them has a big O notation of O(n<sup>2</sup>) while the other has a big O notation of O(n), then I should choose to implement the one with the O(n) notation.   The whole point of this article is show why that isn’t always true, and to explore how one might go about selecting the most efficient option for implementation.   I will specify <em>computer programming</em> functions when discussing actual code so as not to confuse them with the <em>mathematical</em> functions we will be looking at below. </p>

<p>In <em>Figure-1</em> below we have a series of functions you can view <a href="https://www.desmos.com/calculator/nuzg9tvbl9" target="_blank">here on desmos.com</a> in more detail.</p>

<p><a href="https://www.desmos.com/calculator/ffzlbv8t4n" target="_blank">
<img src="https://i.imgur.com/qxVkPQr.png" alt="figure one" />
</a></p>

<p>The takeaway here is that we can observe two mathematical functions with a big O notation of <em>O(n<sup>2</sup>)</em> that scale worse than the function with a big O notation of <em>O(n!)</em>.   At the same time, we can also see three mathematical functions where the opposite is true.   Now to be sure, all of the <em>O(n<sup>2</sup>)</em> functions will scale better than the single <em>O(n!)</em> function <strong><em>if</em></strong> all of the the x and y values (on the grid) were to go to infinity.   Now imagine two computer programming functions that followed trajectories similar to what is shown above; in such a circumstance, the computer programming function with the optimal big O notation might not be the most efficient one within the space shown. </p>

<p>Now let’s consider the following python pseudocode in figure 2 below:</p>

<p><img src="https://imgur.com/2eJX9KL" alt="figure two" /></p>

<p>Here we see what can happen when conditionals are introduced; the big O of our <code class="language-plaintext highlighter-rouge">python_function()</code> is basically useless and doesn’t tell us anything about our overall performance.  This is because the big O here is only relevant when the input variable (x) has a value greater than or equal to 10, and is also less than 20, and yet the <code class="language-plaintext highlighter-rouge">python_function()</code> accepts values above and below that range.    A rule of thumb here is that: <em>if and when a function contains conditionals that evaluate its own input variable, the big O notation of that function might not be useful for evaluating efficiency</em>.   Instead, we would want to valuate the subcomponents separately</p>

<p>Now let’s take a look at a bird’s eye view of big O notation in figure 3 below:</p>

<p><a href="https://www.desmos.com/calculator/odi6muvmzs" target="_blank">
<img src="https://i.imgur.com/bXoHRT0.png" alt="figure three" />
</a></p>

<p>This is similar to Jon Krohn’s Microsoft-paint-sketch introduced at the beginning of this blog post, but it is a bit more comprehensive and contains more big O notations.   You can view these same math functions <a href="https://www.desmos.com/calculator/wanscrgyzq" target="_blank">here on desmos.com</a> where you can zoom in and out to see how they better relate to each other. </p>

<p>There are already several great articles describing the big O notations for various programming operations.   Don Cowan has an excellent <a href="https://www.donkcowan.com/blog/2013/5/11/big-o-notation" target="_blank">summary table</a> and Şahin Arslan has some great <a href="https://dev.to/humblecoder00/comprehensive-big-o-notation-guide-in-plain-english-using-javascript-3n6m" target="_blank">JavaScript example code</a>.   Usman Malik has a <a href="https://stackabuse.com/big-o-notation-and-algorithm-analysis-with-python-examples/" target="_blank">similar article with Python code</a> as well.   We will not repeat this material here but instead try and answer the question: <em>what if I have multiple computer programming functions that do the same thing, and I’m trying to find the one that is the most efficient, should I choose the one that has the best (i.e. most efficient) big O notation?</em>   Well no, not necessary, and in fact, the big O might not help you at all.  To se what I mean let’s look at figure 4:</p>

<p><a href="https://www.desmos.com/calculator/43dmn99elk" target="_blank">
<img src="https://i.imgur.com/R1KMZ4u.png" alt="figure four" />
</a></p>

<p><a href="https://www.desmos.com/calculator/1hqyvyveh6" target="_blank">
<img src="https://i.imgur.com/1fePG6a.png" alt="figure five" />
</a></p>

<p>OK, so at this scale we start to see some problems.   In the space shown above, the (math) functions are intersecting each other all over the place, let’s see what the implications of that are in figure 5 below:</p>

<p><img src="https://i.imgur.com/63wCMJh.png" alt="figure six" /></p>

<p>So here we see that at this scale, different big O notations have efficiencies that are at variance with what they would be at, say, infinity.   Now how would we find out the different efficiencies for multiple functions, and would we use big O?   Let’s look at figure six below.  In figure 6 we have different (competing) functions plotted out to see which one is the best for our purposes:</p>

<p><img src="https://i.imgur.com/OFGpLrF.png" alt="figure seven" /></p>

<p>What the heck are we in need of a stochastic analysis for!?   Let’s say we had a function to analyze different human beings based on their individual height, and that we passed these heights into a function that looks something like this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">python_human_height_function_one</span><span class="p">(</span><span class="n">height</span><span class="p">):</span>
</code></pre></div></div>
<p>In this situation, we already know that <a href="https://ourworldindata.org/human-height#height-is-normally-distributed" target="_blank">human height is normally distributed</a>, and so if we had to choose between two functions that accomplish the same calculation on a human height, but that do so using different methods (and that also have different big O notations) we would want to know which of the two functions works best with human heights that are likely to be entered into the function.   For example, I know some people are taller than 7 feet but there are very few such people, and thus, I wouldn’t choose a function that worked faster for people above 7 feet, but much slower for people under 5 because I’m more likely to have the latter than I am the former. </p>

<p>The truth is that when your trying to choose what function to use among a competing set of functions, the big O notation really doesn’t tell you much at all in terms of which function you should choose, <strong><em>this is because the input variables you’ll be passing to those functions will not necessarily result in the y-axis reaching infinity</em></strong>.   Shen Huang found something along these lines when he discovered a function with a big O of <em>O(n &amp; log(n))</em> that was slower than another function doing the same work, but having a big O of <em>O(n<sup>2</sup>)</em>.   The raw code for his two functions is available <a href="https://trinket.io/python/87a3166026" target="_blank">here</a> and you can read about it at the bottom of this blog post <a href="https://www.freecodecamp.org/news/big-o-notation-why-it-matters-and-why-it-doesnt-1674cfa8a23c/#Why-BigO-doesn%E2%80%99t-matter" target="_blank">here</a> under subsection 7: <strong><em>Why Big O doesn’t matter</em></strong> . </p>

<p>In the next figure, we see a more accurate depiction of what you would get if you were to do this sort-of experiment in a real world setting:</p>

<p><img src="https://i.imgur.com/JyToYfz.png" alt="figure eight" /></p>

<p>You can use an online tool like <a href="https://www.geogebra.org/" target="_blank">GeoGebra</a> to plot your function performance and then do either <a href="https://www.youtube.com/watch?v=TmYl6k4e_AE" target="_blank">liner regression or curve fitting</a>, depending on what you need.   For example, the <code class="language-plaintext highlighter-rouge">third_function</code> above (green path) would need some curve fitting because it angles sharply (around the point where x=26.5) whereas the <code class="language-plaintext highlighter-rouge">second_function</code> (blue path) would call for some linear regression because it is more or less a straight line.   The latter has a linear trajectory whereas the former has polynomial one.   The <code class="language-plaintext highlighter-rouge">first_function</code> appears to follow an exponential path but within the bounded area we are operating on it, it is pretty close to straight as well so you could do linear regression on the red dots and then use the subsequent line to find the intersection with the <code class="language-plaintext highlighter-rouge">second_function</code>(blue path) line. </p>
:ET